---
description: 'В FreeBSD инфраструктура GEOM предоставляет доступ и управление классами, такими как главная загрузочная запись (MBR) и метки BSD, через использование провайдеров или устройств дисков в `/dev`.'
next: books/handbook/zfs
params:
  path: /books/handbook/geom/
part: 'Часть III. Администрирование системы'
prev: books/handbook/disks
showBookMenu: true
tags: ["GEOM", "RAID", "RAID0", "RAID1", "RAID3", "Striping", "bsdlabel", "newfs", "labelling", "UFS", "journaling"]
title: 'Глава 21. GEOM: Модульная инфраструктура трансформации дискового пространства'
weight: 25
---

[[geom]]
= GEOM: Модульная инфраструктура трансформации дискового пространства
:doctype: book
:toc: macro
:toclevels: 1
:icons: font
:sectnums:
:sectnumlevels: 6
:sectnumoffset: 21
:partnums:
:source-highlighter: rouge
:experimental:
:images-path: books/handbook/geom/

ifdef::env-beastie[]
ifdef::backend-html5[]
:imagesdir: ../../../../images/{images-path}
endif::[]
ifndef::book[]
include::shared/authors.adoc[]
include::shared/mirrors.adoc[]
include::shared/releases.adoc[]
include::shared/attributes/attributes-{{% lang %}}.adoc[]
include::shared/{{% lang %}}/teams.adoc[]
include::shared/{{% lang %}}/mailing-lists.adoc[]
include::shared/{{% lang %}}/urls.adoc[]
toc::[]
endif::[]
ifdef::backend-pdf,backend-epub3[]
include::../../../../../shared/asciidoctor.adoc[]
endif::[]
endif::[]

ifndef::env-beastie[]
toc::[]
include::../../../../../shared/asciidoctor.adoc[]
endif::[]

[[geom-synopsis]]
== Обзор

В FreeBSD система GEOM обеспечивает доступ и управление классами, такими как главная загрузочная запись (MBR) и метки BSD, через использование провайдеров или дисковых устройств в [.filename]#/dev#. Поддерживая различные конфигурации программного RAID, GEOM прозрачно предоставляет доступ операционной системе и системным утилитам.

В этой главе рассматривается использование дисков в рамках системы GEOM в FreeBSD. Это включает основные утилиты управления RAID, которые используют данную систему для настройки. Данная глава не является исчерпывающим руководством по конфигурациям RAID и рассматривает только поддерживаемые GEOM классификации RAID.

Прочитав эту главу, вы будете знать:

* Какой тип поддержки RAID доступен через GEOM.
* Как использовать базовые утилиты для настройки, обслуживания и управления различными уровнями RAID.
* Как зеркалировать, чередовать, шифровать и удалённо подключать дисковые устройства с помощью GEOM.
* Как устранять неполадки с дисками, подключенными к системе GEOM.

Прежде чем читать эту главу, вы должны:

* Понимание того, как FreeBSD работает с дисковыми устройствами (crossref:disks[disks,Хранение данных]).
* Знать, как настроить и установить новый ядро (crossref:kernelconfig[kernelconfig,Настройка ядра FreeBSD]).

[[geom-striping]]
== RAID0 - Striping

Чередование объединяет несколько дисков в один том. Чередование может быть выполнено с использованием аппаратных RAID-контроллеров. Подсистема GEOM предоставляет программную поддержку чередования дисков, также известного как RAID0, без необходимости в RAID-контроллере.

В RAID0 данные разбиваются на блоки, которые записываются на все диски массива. Как показано на следующей иллюстрации, вместо ожидания записи 256 КБ на один диск, RAID0 может одновременно записать по 64 КБ на каждый из четырех дисков массива, обеспечивая высокую производительность ввода-вывода. Эту производительность можно дополнительно повысить, используя несколько контроллеров дисков.

image::striping.png["Иллюстрация чередования дисков"]

Каждый диск в дисковой последовательности RAID0 должен быть одинакового размера, поскольку запросы ввода-вывода чередуются для чтения или записи на несколько дисков параллельно.

[NOTE]
====
RAID0 _не_ обеспечивает избыточности. Это означает, что если один диск в массиве выйдет из строя, все данные на дисках будут потеряны. Если данные важны, реализуйте стратегию резервного копирования, которая регулярно сохраняет резервные копии на удалённую систему или устройство.
====

Процесс создания программного RAID0 на основе GEOM в системе FreeBSD с использованием обычных дисков выглядит следующим образом. После создания дисковой последовательности обратитесь к man:gstripe[8] для получения дополнительной информации о том, как управлять существующей дисковой последовательностью.

[.procedure]
****
*Procedure: Creating a Stripe of Unformatted ATA Disks*

. Загрузите модуль [.filename]#geom_stripe.ko#:
+
[source, shell]
....
# kldload geom_stripe
....

. Убедитесь, что существует подходящая точка монтирования. Если этот том будет использоваться как корневой раздел, временно используйте другую точку монтирования, например, [.filename]#/mnt#.
. Определите имена устройств для дисков, которые будут объединены в дисковую последовательность, и создайте новое устройство для последовательности. Например, для объединения в последовательность двух неиспользуемых и неразмеченных ATA-дисков с именами устройств [.filename]#/dev/ad2# и [.filename]#/dev/ad3#:
+
[source, shell]
....
# gstripe label -v st0 /dev/ad2 /dev/ad3
Metadata value stored on /dev/ad2.
Metadata value stored on /dev/ad3.
Done.
....

. Запишите стандартную метку, также известную как таблица разделов, на новый том и установите код начальной загрузки по умолчанию:
+
[source, shell]
....
# bsdlabel -wB /dev/stripe/st0
....

. Этот процесс должен создать два дополнительных устройства в [.filename]#/dev/stripe# помимо [.filename]#st0# — [.filename]#st0a# и [.filename]#st0c#. На этом этапе можно создать файловую систему UFS на [.filename]#st0a# с помощью команды `newfs`:
+
[source, shell]
....
# newfs -U /dev/stripe/st0a
....
+
По экрану пройдёт множество чисел, и через несколько секунд процесс завершится. Том создан и готов к монтированию.
. Чтобы вручную смонтировать созданную дисковую последовательность:
+
[source, shell]
....
# mount /dev/stripe/st0a /mnt
....

. Для автоматического монтирования этой чередующейся файловой системы во время загрузки добавьте информацию о томе в [.filename]#/etc/fstab#. В этом примере создается постоянная точка монтирования с именем [.filename]#stripe#:
+
[source, shell]
....
# mkdir /stripe
# echo "/dev/stripe/st0a /stripe ufs rw 2 2" \
>> /etc/fstab
....

. Модуль [.filename]#geom_stripe.ko# также должен автоматически загружаться при инициализации системы, добавив строку в [.filename]#/boot/loader.conf#:
+
[source, shell]
....
# echo 'geom_stripe_load="YES"' >> /boot/loader.conf
....
****

[[geom-mirror]]
== RAID1 - Зеркалирование

RAID1, или _зеркалирование_, — это техника записи одних и тех же данных на несколько дисковых накопителей. Зеркала обычно используются для защиты от потери данных из-за выхода диска из строя. Каждый диск в зеркале содержит идентичную копию данных. При отказе одного из дисков зеркало продолжает работать, предоставляя данные с оставшихся исправных дисков. Компьютер продолжает функционировать, а администратор имеет возможность заменить вышедший из строя диск без прерывания работы пользователей.

В этих примерах показаны две распространённые ситуации. Первый пример создаёт зеркало из двух новых дисков и использует его для замены существующего одиночного диска. Второй пример создаёт зеркало на одном новом диске, копирует на него данные со старого диска, а затем добавляет старый диск в зеркало. Хотя эта процедура немного сложнее, она требует только одного нового диска.

Традиционно два диска в зеркале имеют одинаковую модель и объем, но man:gmirror[8] не требует этого. Зеркала, созданные с разными дисками, будут иметь объем, равный объему наименьшего диска в зеркале. Дополнительное место на более крупных дисках останется неиспользуемым. Диски, добавляемые в зеркало позже, должны иметь объем не меньше, чем у наименьшего диска, уже находящегося в зеркале.

[WARNING]
====
Приведённые здесь процедуры зеркалирования являются неразрушающими, но, как и при любых операциях с дисками, сначала создайте полную резервную копию.
====

[WARNING]
====
В этих процедурах используется man:dump[8] для копирования файловых систем, однако он не работает с файловыми системами, использующими журналирование мягких обновлений. Информацию о том, как обнаружить и отключить журналирование мягких обновлений, смотрите в man:tunefs[8].
====

[[geom-mirror-metadata]]
=== Проблемы с метаданными

Многие дисковые системы хранят метаданные в конце каждого диска. Старые метаданные следует стереть перед повторным использованием диска для зеркала. Большинство проблем вызвано двумя конкретными типами оставшихся метаданных: таблицами разделов GPT и старыми метаданными от предыдущего зеркала.

Метаданные GPT можно удалить с помощью man:gpart[8]. В этом примере удаляются как основная, так и резервная таблицы разделов GPT с диска [.filename]#ada8#:

[source, shell]
....
# gpart destroy -F ada8
....

Диск может быть удален из активного зеркала, а его метаданные стерты за один шаг с помощью man:gmirror[8]. В этом примере диск [.filename]#ada8# удаляется из активного зеркала [.filename]#gm4#:

[source, shell]
....
# gmirror remove gm4 ada8
....

Если зеркало не запущено, но старые метаданные зеркала остались на диске, используйте `gmirror clear` для их удаления:

[source, shell]
....
# gmirror clear ada8
....

man:gmirror[8] хранит один блок метаданных в конце диска. Поскольку схемы разделов GPT также хранят метаданные в конце диска, зеркалирование целых GPT-дисков с помощью man:gmirror[8] не рекомендуется. Здесь используется разделение MBR, так как оно хранит таблицу разделов только в начале диска и не конфликтует с метаданными зеркала.

[[geom-mirror-two-new-disks]]
=== Создание зеркала с двумя новыми дисками

В этом примере FreeBSD уже установлена на одном диске [.filename]#ada0#. К системе подключены два новых диска — [.filename]#ada1# и [.filename]#ada2#. На этих дисках будет создано новое зеркало, которое заменит старый одиночный диск.

Модуль ядра [.filename]#geom_mirror.ko# должен быть либо встроен в ядро, либо загружен при загрузке или во время работы. Вручную загрузите модуль ядра сейчас:

[source, shell]
....
# gmirror load
....

Создайте зеркало с двумя новыми дисками:

[source, shell]
....
# gmirror label -v gm0 /dev/ada1 /dev/ada2
....

[.filename]#gm0# — это выбранное пользователем имя устройства, назначенное новому зеркалу. После запуска зеркала это имя устройства появляется в [.filename]#/dev/mirror/#.

Теперь таблицы разделов MBR и bsdlabel можно создавать на зеркале с помощью man:gpart[8]. В этом примере используется традиционная структура файловой системы с разделами для [.filename]#/#, swap, [.filename]#/var#, [.filename]#/tmp# и [.filename]#/usr#. Также подойдёт одиночный раздел [.filename]#/# и раздел подкачки.

Разделы на зеркале не обязательно должны быть такого же размера, как и на существующем диске, но они должны быть достаточно большими, чтобы вместить все данные, уже находящиеся на [.filename]#ada0#.

[source, shell]
....
# gpart create -s MBR mirror/gm0
# gpart add -t freebsd -a 4k mirror/gm0
# gpart show mirror/gm0
=>       63  156301423  mirror/gm0  MBR  (74G)
         63         63                    - free -  (31k)
        126  156301299                 1  freebsd  (74G)
  156301425         61                    - free -  (30k)
....

[source, shell]
....
# gpart create -s BSD mirror/gm0s1
# gpart add -t freebsd-ufs  -a 4k -s 2g mirror/gm0s1
# gpart add -t freebsd-swap -a 4k -s 4g mirror/gm0s1
# gpart add -t freebsd-ufs  -a 4k -s 2g mirror/gm0s1
# gpart add -t freebsd-ufs  -a 4k -s 1g mirror/gm0s1
# gpart add -t freebsd-ufs  -a 4k mirror/gm0s1
# gpart show mirror/gm0s1
=>        0  156301299  mirror/gm0s1  BSD  (74G)
          0          2                      - free -  (1.0k)
          2    4194304                   1  freebsd-ufs  (2.0G)
    4194306    8388608                   2  freebsd-swap (4.0G)
   12582914    4194304                   4  freebsd-ufs  (2.0G)
   16777218    2097152                   5  freebsd-ufs  (1.0G)
   18874370  137426928                   6  freebsd-ufs  (65G)
  156301298          1                      - free -  (512B)
....

Сделайте зеркало загружаемым, установив загрузочный код в MBR и bsdlabel, а также настроив активный раздел:

[source, shell]
....
# gpart bootcode -b /boot/mbr mirror/gm0
# gpart set -a active -i 1 mirror/gm0
# gpart bootcode -b /boot/boot mirror/gm0s1
....

Отформатируйте файловые системы на новом зеркале, включив мягкие обновления.

[source, shell]
....
# newfs -U /dev/mirror/gm0s1a
# newfs -U /dev/mirror/gm0s1d
# newfs -U /dev/mirror/gm0s1e
# newfs -U /dev/mirror/gm0s1f
....

Файловые системы с исходного диска [.filename]#ada0# теперь можно скопировать на зеркало с помощью man:dump[8] и man:restore[8].

[source, shell]
....
# mount /dev/mirror/gm0s1a /mnt
# dump -C16 -b64 -0aL -f - / | (cd /mnt && restore -rf -)
# mount /dev/mirror/gm0s1d /mnt/var
# mount /dev/mirror/gm0s1e /mnt/tmp
# mount /dev/mirror/gm0s1f /mnt/usr
# dump -C16 -b64 -0aL -f - /var | (cd /mnt/var && restore -rf -)
# dump -C16 -b64 -0aL -f - /tmp | (cd /mnt/tmp && restore -rf -)
# dump -C16 -b64 -0aL -f - /usr | (cd /mnt/usr && restore -rf -)
....

Отредактируйте файл [.filename]#/mnt/etc/fstab#, чтобы он указывал на файловые системы нового зеркала:

[.programlisting]
....
# Device		Mountpoint	FStype	Options	Dump	Pass#
/dev/mirror/gm0s1a	/		ufs	rw	1	1
/dev/mirror/gm0s1b	none		swap	sw	0	0
/dev/mirror/gm0s1d	/var		ufs	rw	2	2
/dev/mirror/gm0s1e	/tmp		ufs	rw	2	2
/dev/mirror/gm0s1f	/usr		ufs	rw	2	2
....

Если модуль ядра [.filename]#geom_mirror.ko# не встроен в ядро, файл [.filename]#/mnt/boot/loader.conf# редактируется для загрузки модуля при старте системы:

[.programlisting]
....
geom_mirror_load="YES"
....

Перезагрузите систему, чтобы проверить новое зеркало и убедиться, что все данные скопированы. BIOS увидит зеркало как два отдельных диска, а не как зеркало. Поскольку диски идентичны, не имеет значения, какой из них выбран для загрузки.

См. crossref:geom[gmirror-troubleshooting, Устранение неполадок], если возникли проблемы с загрузкой. Отключение питания и извлечение исходного диска [.filename]#ada0# позволит сохранить его в качестве автономной резервной копии.

При использовании зеркало будет вести себя так же, как исходный одиночный диск.

[[geom-mirror-existing-drive]]
=== Создание зеркала с существующим диском

В этом примере FreeBSD уже установлена на одном диске [.filename]#ada0#. К системе подключён новый диск [.filename]#ada1#. На новом диске будет создано однодисковое зеркало, существующая система скопирована на него, после чего старый диск будет добавлен в зеркало. Эта несколько сложная процедура необходима, потому что `gmirror` требует размещения 512-байтового блока метаданных в конце каждого диска, а на существующем [.filename]#ada0# обычно всё пространство уже занято.

Загрузите модуль ядра [.filename]#geom_mirror.ko#:

[source, shell]
....
# gmirror load
....

Проверьте размер носителя исходного диска с помощью `diskinfo`:

[source, shell]
....
# diskinfo -v ada0 | head -n3
/dev/ada0
        512             # sectorsize
        1000204821504   # mediasize in bytes (931G)
....

Создайте зеркало на новом диске. Чтобы убедиться, что объем зеркала не превышает объем исходного диска [.filename]#ada0#, используется man:gnop[8] для создания виртуального диска того же размера. Этот диск не хранит данных, а служит только для ограничения размера зеркала. Когда man:gmirror[8] создает зеркало, он ограничит объем до размера [.filename]#gzero.nop#, даже если новый диск [.filename]#ada1# имеет больше места. Обратите внимание, что число _1000204821504_ во второй строке соответствует размеру носителя [.filename]#ada0#, показанному командой `diskinfo` выше.

[source, shell]
....
# geom zero load
# gnop create -s 1000204821504 gzero
# gmirror label -v gm0 gzero.nop ada1
# gmirror forget gm0
....

Поскольку [.filename]#gzero.nop# не хранит никаких данных, зеркало не считает его подключенным. Зеркалу указывается «забыть» неподключенные компоненты, удаляя ссылки на [.filename]#gzero.nop#. В результате получается зеркальное устройство, содержащее только один диск — [.filename]#ada1#.

После создания [.filename]#gm0# просмотрите таблицу разделов на [.filename]#ada0#. Этот вывод сделан для диска объемом 1 ТБ. Если в конце диска осталось нераспределенное пространство, содержимое можно скопировать напрямую с [.filename]#ada0# на новое зеркало.

Однако, если вывод показывает, что всё пространство на диске занято, как в следующем примере, то для метаданных зеркала размером 512 байт в конце диска не остаётся места.

[source, shell]
....
# gpart show ada0
=>        63  1953525105        ada0  MBR  (931G)
          63  1953525105           1  freebsd  [active]  (931G)
....

В этом случае необходимо отредактировать таблицу разделов, уменьшив ёмкость на один сектор для [.filename]#mirror/gm0#. Процедура будет описана далее.

В любом случае таблицы разделов на основном диске следует сначала скопировать с помощью `gpart backup` и `gpart restore`.

[source, shell]
....
# gpart backup ada0 > table.ada0
# gpart backup ada0s1 > table.ada0s1
....

Эти команды создают два файла, [.filename]#table.ada0# и [.filename]#table.ada0s1#. Этот пример приведен для диска объемом 1 ТБ:

[source, shell]
....
# cat table.ada0
MBR 4
1 freebsd         63 1953525105   [active]
....

[source, shell]
....
# cat table.ada0s1
BSD 8
1  freebsd-ufs          0    4194304
2 freebsd-swap    4194304   33554432
4  freebsd-ufs   37748736   50331648
5  freebsd-ufs   88080384   41943040
6  freebsd-ufs  130023424  838860800
7  freebsd-ufs  968884224  984640881
....

Если в конце диска не отображается свободное пространство, размер и раздела, и последнего раздела должен быть уменьшен на один сектор. Отредактируйте два файла, уменьшив размер и раздела, и последнего раздела на единицу. Это последние числа в каждом списке.

[source, shell]
....
# cat table.ada0
MBR 4
1 freebsd         63 1953525104   [active]
....

[source, shell]
....
# cat table.ada0s1
BSD 8
1  freebsd-ufs          0    4194304
2 freebsd-swap    4194304   33554432
4  freebsd-ufs   37748736   50331648
5  freebsd-ufs   88080384   41943040
6  freebsd-ufs  130023424  838860800
7  freebsd-ufs  968884224  984640880
....

Если хотя бы один сектор в конце диска был нераспределен, эти два файла можно использовать без изменений.

Теперь восстановите таблицу разделов в [.filename]#mirror/gm0#:

[source, shell]
....
# gpart restore mirror/gm0 < table.ada0
# gpart restore mirror/gm0s1 < table.ada0s1
....

Проверьте таблицу разделов с помощью `gpart show`. В этом примере [.filename]#gm0s1a# для [.filename]#/#, [.filename]#gm0s1d# для [.filename]#/var#, [.filename]#gm0s1e# для [.filename]#/usr#, [.filename]#gm0s1f# для [.filename]#/data1# и [.filename]#gm0s1g# для [.filename]#/data2#.

[source, shell]
....
# gpart show mirror/gm0
=>        63  1953525104  mirror/gm0  MBR  (931G)
          63  1953525042           1  freebsd  [active]  (931G)
  1953525105          62              - free -  (31k)

# gpart show mirror/gm0s1
=>         0  1953525042  mirror/gm0s1  BSD  (931G)
           0     2097152             1  freebsd-ufs  (1.0G)
     2097152    16777216             2  freebsd-swap  (8.0G)
    18874368    41943040             4  freebsd-ufs  (20G)
    60817408    20971520             5  freebsd-ufs  (10G)
    81788928   629145600             6  freebsd-ufs  (300G)
   710934528  1242590514             7  freebsd-ufs  (592G)
  1953525042          63                - free -  (31k)
....

Как срез, так и последний раздел должны иметь как минимум один свободный блок в конце диска.

Создайте файловые системы на этих новых разделах. Количество разделов может варьироваться в соответствии с исходным диском, [.filename]#ada0#.

[source, shell]
....
# newfs -U /dev/mirror/gm0s1a
# newfs -U /dev/mirror/gm0s1d
# newfs -U /dev/mirror/gm0s1e
# newfs -U /dev/mirror/gm0s1f
# newfs -U /dev/mirror/gm0s1g
....

Сделайте зеркало загружаемым, установив загрузочный код в MBR и bsdlabel, а также настроив активный раздел:

[source, shell]
....
# gpart bootcode -b /boot/mbr mirror/gm0
# gpart set -a active -i 1 mirror/gm0
# gpart bootcode -b /boot/boot mirror/gm0s1
....

Настройте [.filename]#/etc/fstab# для использования новых разделов на зеркале. Сначала создайте резервную копию этого файла, скопировав его в [.filename]#/etc/fstab.orig#.

[source, shell]
....
# cp /etc/fstab /etc/fstab.orig
....

Отредактируйте файл [.filename]#/etc/fstab#, заменив [.filename]#/dev/ada0# на [.filename]#mirror/gm0#.

[.programlisting]
....
# Device		Mountpoint	FStype	Options	Dump	Pass#
/dev/mirror/gm0s1a	/		ufs	rw	1	1
/dev/mirror/gm0s1b	none		swap	sw	0	0
/dev/mirror/gm0s1d	/var		ufs	rw	2	2
/dev/mirror/gm0s1e	/usr		ufs	rw	2	2
/dev/mirror/gm0s1f	/data1		ufs	rw	2	2
/dev/mirror/gm0s1g	/data2		ufs	rw	2	2
....

Если модуль ядра [.filename]#geom_mirror.ko# не встроен в ядро, отредактируйте [.filename]#/boot/loader.conf# для его загрузки при старте системы:

[.programlisting]
....
geom_mirror_load="YES"
....

Файловые системы с исходного диска теперь можно скопировать на зеркало с помощью man:dump[8] и man:restore[8]. Каждая файловая система, сохранённая с помощью `dump -L`, сначала создаст снимок, что может занять некоторое время.

[source, shell]
....
# mount /dev/mirror/gm0s1a /mnt
# dump -C16 -b64 -0aL -f - /    | (cd /mnt && restore -rf -)
# mount /dev/mirror/gm0s1d /mnt/var
# mount /dev/mirror/gm0s1e /mnt/usr
# mount /dev/mirror/gm0s1f /mnt/data1
# mount /dev/mirror/gm0s1g /mnt/data2
# dump -C16 -b64 -0aL -f - /usr | (cd /mnt/usr && restore -rf -)
# dump -C16 -b64 -0aL -f - /var | (cd /mnt/var && restore -rf -)
# dump -C16 -b64 -0aL -f - /data1 | (cd /mnt/data1 && restore -rf -)
# dump -C16 -b64 -0aL -f - /data2 | (cd /mnt/data2 && restore -rf -)
....

Перезагрузите систему, загрузившись с [.filename]#ada1#. Если всё работает правильно, система загрузится с [.filename]#mirror/gm0#, который теперь содержит те же данные, что ранее были на [.filename]#ada0#. Обратитесь к разделу crossref:geom[gmirror-troubleshooting, Устранение неполадок], если возникли проблемы с загрузкой.

На этом этапе зеркало всё ещё состоит только из одного диска [.filename]#ada1#.

После успешной загрузки с [.filename]#mirror/gm0# последним шагом будет добавление [.filename]#ada0# в зеркало.

[IMPORTANT]
====
Когда диск [.filename]#ada0# добавляется в зеркало, его прежнее содержимое будет перезаписано данными из зеркала. Убедитесь, что [.filename]#mirror/gm0# имеет такое же содержимое, как [.filename]#ada0#, перед добавлением [.filename]#ada0# в зеркало. Если данные, ранее скопированные с помощью man:dump[8] и man:restore[8], не идентичны тому, что было на [.filename]#ada0#, верните [.filename]#/etc/fstab# к монтированию файловых систем на [.filename]#ada0#, перезагрузите систему и начните всю процедуру заново.
====

[source, shell]
....
# gmirror insert gm0 ada0
GEOM_MIRROR: Device gm0: rebuilding provider ada0
....

Синхронизация между двумя дисками начнется немедленно. Для просмотра прогресса используйте команду `gmirror status`.

[source, shell]
....
# gmirror status
      Name    Status  Components
mirror/gm0  DEGRADED  ada1 (ACTIVE)
                      ada0 (SYNCHRONIZING, 64%)
....

Через некоторое время синхронизация завершится.

[source, shell]
....
GEOM_MIRROR: Device gm0: rebuilding provider ada0 finished.
# gmirror status
      Name    Status  Components
mirror/gm0  COMPLETE  ada1 (ACTIVE)
                      ada0 (ACTIVE)
....

[.filename]#mirror/gm0# теперь состоит из двух дисков [.filename]#ada0# и [.filename]#ada1#, и их содержимое автоматически синхронизируется между собой. При использовании [.filename]#mirror/gm0# будет вести себя так же, как исходный одиночный диск.

[[gmirror-troubleshooting]]
=== Устранение неполадок

Если система больше не загружается, возможно, потребуется изменить настройки BIOS для загрузки с одного из новых зеркалированных дисков. Для загрузки можно использовать любой из зеркальных дисков, так как они содержат идентичные данные.

Если загрузка останавливается с таким сообщением, значит, что-то не так с зеркальным устройством:

[source, shell]
....
Mounting from ufs:/dev/mirror/gm0s1a failed with error 19.

Loader variables:
  vfs.root.mountfrom=ufs:/dev/mirror/gm0s1a
  vfs.root.mountfrom.options=rw

Manual root filesystem specification:
  <fstype>:<device> [options]
      Mount <device> using filesystem <fstype>
      and with the specified (optional) option list.

    e.g. ufs:/dev/da0s1a
        zfs:tank
        cd9660:/dev/acd0 ro
          (which is equivalent to: mount -t cd9660 -o ro /dev/acd0 /)

  ?               List valid disk boot devices
  .               Yield 1 second (for background tasks)
  <empty line>    Abort manual input

mountroot>
....

Проблема может быть вызвана тем, что забыли загрузить модуль [.filename]#geom_mirror.ko# , не добавив его в [.filename]#/boot/loader.conf#. Чтобы исправить её, загрузитесь с установочного носителя FreeBSD и выберите `Shell` при первом запросе. Затем загрузите модуль mirror и смонтируйте зеркальное устройство:

[source, shell]
....
# gmirror load
# mount /dev/mirror/gm0s1a /mnt
....

Отредактируйте файл [.filename]#/mnt/boot/loader.conf#, добавив строку для загрузки модуля mirror:

[.programlisting]
....
geom_mirror_load="YES"
....

Сохраните файл и перезагрузите систему.

Другие проблемы, вызывающие `ошибку 19`, требуют больше усилий для исправления. Хотя система должна загружаться с [.filename]#ada0#, появится ещё один запрос на выбор оболочки, если [.filename]#/etc/fstab# указан неверно. Введите `ufs:/dev/ada0s1a` в строке загрузчика и нажмите kbd:[Enter]. Отмените изменения в [.filename]#/etc/fstab#, затем смонтируйте файловые системы с исходного диска ([.filename]#ada0#) вместо зеркала. Перезагрузите систему и повторите процедуру снова.

[source, shell]
....
Enter full pathname of shell or RETURN for /bin/sh:
# cp /etc/fstab.orig /etc/fstab
# reboot
....

=== Восстановление после отказа диска

Преимущество зеркалирования дисков заключается в том, что при отказе одного диска зеркало не теряет данных. В приведённом примере, если [.filename]#ada0# выйдет из строя, зеркало продолжит работать, предоставляя данные с оставшегося рабочего диска [.filename]#ada1#.

Для замены вышедшего из строя диска завершите работу системы и физически замените неисправный диск новым диском равной или большей ёмкости. Производители используют довольно произвольные значения при указании ёмкости дисков в гигабайтах, и единственный способ точно убедиться в этом — сравнить общее количество секторов, отображаемое командой `diskinfo -v`. Диск с большей ёмкостью, чем у зеркала, будет работать, хотя дополнительное пространство на новом диске использоваться не будет.

После перезагрузки компьютера зеркало будет работать в "деградированном" режиме с одним диском. Необходимо указать зеркалу забыть диски, которые в данный момент не подключены:

[source, shell]
....
# gmirror forget gm0
....

Любые старые метаданные должны быть удалены с заменяемого диска, следуя инструкциям в crossref:geom[geom-mirror-metadata, Проблемы с метаданными]. Затем заменяющий диск, [.filename]#ada4# в данном примере, добавляется в зеркало:

[source, shell]
....
# gmirror insert gm0 /dev/ada4
....

Синхронизация начинается при установке нового диска в зеркало. Этот процесс копирования данных зеркала на новый диск может занять некоторое время. Производительность зеркала значительно снизится во время копирования, поэтому добавление новых дисков лучше выполнять при низкой нагрузке на компьютер.

Ход выполнения можно отслеживать с помощью `gmirror status`, который показывает диски, находящиеся в процессе синхронизации, и процент завершения. Во время повторной синхронизации статус будет `DEGRADED`, а по завершении процесса изменится на `COMPLETE`.

[[geom-raid3]]
== RAID3 - чередование на уровне байтов с выделенной четностью

RAID3 — это метод объединения нескольких дисков в один том с выделенным диском четности. В системе RAID3 данные разбиваются на байты, которые записываются на все диски массива, за исключением одного диска, выполняющего роль выделенного диска четности. Это означает, что операции чтения в RAID3 обращаются ко всем дискам массива. Производительность может быть повышена за счет использования нескольких контроллеров дисков. Массив RAID3 обеспечивает отказоустойчивость на уровне одного диска, а его емкость составляет 1 - 1/n от общей емкости всех дисков массива, где n — количество жестких дисков в массиве. Такая конфигурация в основном подходит для хранения данных большого объема, таких как мультимедийные файлы.

Для создания массива RAID3 требуется как минимум 3 физических жестких диска. Каждый диск должен быть одинакового размера, так как запросы ввода-вывода чередуются для чтения или записи на несколько дисков параллельно. Кроме того, из-за особенностей RAID3 количество дисков должно быть равно 3, 5, 9, 17 и так далее, то есть соответствовать формуле 2^n + 1.

Этот раздел демонстрирует, как создать программный RAID3 в системе FreeBSD.

[NOTE]
====
Хотя теоретически возможно загружаться с массива RAID3 в FreeBSD, такая конфигурация встречается редко и не рекомендуется.
====

=== Создание выделенного массива RAID3

В FreeBSD поддержка RAID3 реализована классом GEOM man:graid3[8]. Создание выделенного массива RAID3 в FreeBSD требует выполнения следующих шагов.

[.procedure]
. Сначала загрузите модуль ядра [.filename]#geom_raid3.ko#, выполнив одну из следующих команд:
+
[source, shell]
....
# graid3 load
....
+
или:
+
[source, shell]
....
# kldload geom_raid3
....

. Убедитесь, что существует подходящая точка монтирования. Эта команда создает новый каталог для использования в качестве точки монтирования:
+
[source, shell]
....
# mkdir /multimedia
....

. Определите имена устройств для дисков, которые будут добавлены в массив, и создайте новое устройство RAID3. Последнее указанное устройство будет использоваться в качестве выделенного диска четности. В этом примере используются три неразделенных ATA-диска: [.filename]#ada1# и [.filename]#ada2# для данных, а [.filename]#ada3# для четности.
+
[source, shell]
....
# graid3 label -v gr0 /dev/ada1 /dev/ada2 /dev/ada3
Metadata value stored on /dev/ada1.
Metadata value stored on /dev/ada2.
Metadata value stored on /dev/ada3.
Done.
....

. Разделите только что созданное устройство [.filename]#gr0# и разместите на нем файловую систему UFS:
+
[source, shell]
....
# gpart create -s GPT /dev/raid3/gr0
# gpart add -t freebsd-ufs /dev/raid3/gr0
# newfs -j /dev/raid3/gr0p1
....
+
По экрану пройдёт множество чисел, и через некоторое время процесс завершится. Том создан и готов к монтированию:
+
[source, shell]
....
# mount /dev/raid3/gr0p1 /multimedia/
....
+
Массив RAID3 теперь готов к использованию.

Для сохранения данной конфигурации после перезагрузки системы требуется дополнительная настройка.

[.procedure]
. Модуль [.filename]#geom_raid3.ko# должен быть загружен перед монтированием массива. Для автоматической загрузки модуля ядра во время инициализации системы добавьте следующую строку в [.filename]#/boot/loader.conf#:
+
[.programlisting]
....
geom_raid3_load="YES"
....

. Следующую информацию о разделе необходимо добавить в [.filename]#/etc/fstab#, чтобы автоматически монтировать файловую систему массива во время загрузки системы:
+
[.programlisting]
....
/dev/raid3/gr0p1	/multimedia	ufs	rw	2	2
....

[[geom-graid]]
== Устройства с программным RAID

Некоторые материнские платы и карты расширения добавляют простые аппаратные компоненты, обычно только ПЗУ, что позволяет компьютеру загружаться с RAID-массива. После загрузки доступ к RAID-массиву обеспечивается программным обеспечением, работающим на основном процессоре компьютера. Такой "аппаратно-ускоренный программный RAID" создаёт массивы, не зависящие от конкретной операционной системы и функционирующие ещё до её загрузки.

Несколько уровней RAID поддерживаются в зависимости от используемого оборудования. Полный список см. в man:graid[8].

man:graid[8] требует наличия модуля ядра [.filename]#geom_raid.ko#, который включён в ядро [.filename]#GENERIC# начиная с FreeBSD 9.1. При необходимости его можно загрузить вручную командой `graid load`.

[[geom-graid-creating]]
=== Создание массива

В устройствах с программным RAID часто есть меню, которое можно вызвать, нажав специальные клавиши при загрузке компьютера. Это меню позволяет создавать и удалять RAID-массивы. man:graid[8] также может создавать массивы напрямую из командной строки.

`graid label` используется для создания нового массива. В данном примере используется материнская плата с чипсетом Intel software RAID, поэтому указан формат метаданных Intel. Новый массив получает метку [.filename]#gm0#, это зеркало (RAID1), и использует диски [.filename]#ada0# и [.filename]#ada1#.

[CAUTION]
====
Некоторое пространство на дисках будет перезаписано при создании нового массива. Предварительно создайте резервную копию существующих данных!
====

[source, shell]
....
# graid label Intel gm0 RAID1 ada0 ada1
GEOM_RAID: Intel-a29ea104: Array Intel-a29ea104 created.
GEOM_RAID: Intel-a29ea104: Disk ada0 state changed from NONE to ACTIVE.
GEOM_RAID: Intel-a29ea104: Subdisk gm0:0-ada0 state changed from NONE to ACTIVE.
GEOM_RAID: Intel-a29ea104: Disk ada1 state changed from NONE to ACTIVE.
GEOM_RAID: Intel-a29ea104: Subdisk gm0:1-ada1 state changed from NONE to ACTIVE.
GEOM_RAID: Intel-a29ea104: Array started.
GEOM_RAID: Intel-a29ea104: Volume gm0 state changed from STARTING to OPTIMAL.
Intel-a29ea104 created
GEOM_RAID: Intel-a29ea104: Provider raid/r0 for volume gm0 created.
....

Проверка состояния показывает, что новое зеркало готово к использованию:

[source, shell]
....
# graid status
   Name   Status  Components
raid/r0  OPTIMAL  ada0 (ACTIVE (ACTIVE))
                  ada1 (ACTIVE (ACTIVE))
....

Устройство массива отображается в [.filename]#/dev/raid/#. Первый массив называется [.filename]#r0#. Дополнительные массивы, если они есть, будут называться [.filename]#r1#, [.filename]#r2# и так далее.

В меню BIOS на некоторых из этих устройств можно создавать массивы со специальными символами в их именах. Чтобы избежать проблем с этими специальными символами, массивам присваиваются простые числовые имена, например, [.filename]#r0#. Для отображения фактических меток, таких как [.filename]#gm0# в примере выше, используйте man:sysctl[8]:

[source, shell]
....
# sysctl kern.geom.raid.name_format=1
....

[[geom-graid-volumes]]
=== Несколько томов

Некоторые устройства программного RAID поддерживают более одного _тома_ в массиве. Томы работают подобно разделам, позволяя разделять пространство на физических дисках и использовать его различными способами. Например, устройства программного RAID от Intel поддерживают два тома. В этом примере создаётся зеркало размером 40 ГБ для безопасного хранения операционной системы, а затем том RAID0 (чередующийся) размером 20 ГБ для быстрого временного хранения:

[source, shell]
....
# graid label -S 40G Intel gm0 RAID1 ada0 ada1
# graid add -S 20G gm0 RAID0
....

Тома появляются как дополнительные записи [.filename]#rX# в каталоге [.filename]#/dev/raid/#. Массив с двумя томами будет отображать [.filename]#r0# и [.filename]#r1#.

См. man:graid[8] для информации о количестве томов, поддерживаемых различными устройствами программного RAID.

[[geom-graid-converting]]
=== Преобразование одиночного диска в зеркало

При определенных условиях возможно преобразовать существующий одиночный диск в массив man:graid[8] без переформатирования. Чтобы избежать потери данных во время преобразования, существующий диск должен соответствовать следующим минимальным требованиям:

* Диск должен быть размечен с использованием схемы разделов MBR. Схемы разделов GPT или другие схемы с метаданными в конце диска будут перезаписаны и повреждены метаданными man:graid[8].
* Для размещения метаданных man:graid[8] в конце диска должно быть достаточно неразмеченного и неиспользуемого пространства. Размер этих метаданных может варьироваться, но самые большие занимают 64 МБ, поэтому рекомендуется иметь как минимум столько свободного места.

Если диск соответствует этим требованиям, начните с создания полной резервной копии. Затем создайте зеркало с одним диском на этом диске:

[source, shell]
....
# graid label Intel gm0 RAID1 ada0 NONE
....

Метаданные `man:graid[8]` были записаны в конец диска в неиспользуемое пространство. Теперь можно вставить второй диск в зеркало:

[source, shell]
....
# graid insert raid/r0 ada1
....

Данные с исходного диска начнут немедленно копироваться на второй диск. Зеркало будет работать в деградировавшем состоянии до завершения копирования.

[[geom-graid-inserting]]
=== Добавление новых дисков к массиву

Диски могут быть добавлены в массив в качестве замены отказавших или отсутствующих дисков. Если нет отказавших или отсутствующих дисков, новый диск становится запасным. Например, добавление нового диска в рабочее зеркало из двух дисков приведет к зеркалу из двух дисков с одним запасным, а не к зеркалу из трех дисков.

В примере зеркального массива данные сразу же начинают копироваться на только что вставленный диск. Любая существующая информация на новом диске будет перезаписана.

[source, shell]
....
# graid insert raid/r0 ada1
GEOM_RAID: Intel-a29ea104: Disk ada1 state changed from NONE to ACTIVE.
GEOM_RAID: Intel-a29ea104: Subdisk gm0:1-ada1 state changed from NONE to NEW.
GEOM_RAID: Intel-a29ea104: Subdisk gm0:1-ada1 state changed from NEW to REBUILD.
GEOM_RAID: Intel-a29ea104: Subdisk gm0:1-ada1 rebuild start at 0.
....

[[geom-graid-removing]]
=== Удаление дисков из массива

Отдельные диски можно навсегда удалить из массива и стереть их метаданные:

[source, shell]
....
# graid remove raid/r0 ada1
GEOM_RAID: Intel-a29ea104: Disk ada1 state changed from ACTIVE to OFFLINE.
GEOM_RAID: Intel-a29ea104: Subdisk gm0:1-[unknown] state changed from ACTIVE to NONE.
GEOM_RAID: Intel-a29ea104: Volume gm0 state changed from OPTIMAL to DEGRADED.
....

[[geom-graid-stopping]]
=== Остановка массива

Массив можно остановить без удаления метаданных с дисков. Массив будет перезапущен при загрузке системы.

[source, shell]
....
# graid stop raid/r0
....

[[geom-graid-status]]
=== Проверка состояния массива

Статус массива можно проверить в любое время. После добавления диска в зеркало в приведённом выше примере данные копируются с исходного диска на новый:

[source, shell]
....
# graid status
   Name    Status  Components
raid/r0  DEGRADED  ada0 (ACTIVE (ACTIVE))
                   ada1 (ACTIVE (REBUILD 28%))
....

Некоторые типы массивов, такие как `RAID0` или `CONCAT`, могут не отображаться в отчете о состоянии при выходе дисков из строя. Чтобы увидеть эти частично неработоспособные массивы, добавьте `-ga`:

[source, shell]
....
# graid status -ga
          Name  Status  Components
Intel-e2d07d9a  BROKEN  ada6 (ACTIVE (ACTIVE))
....

[[geom-graid-deleting]]
=== Удаление массивов

Массивы уничтожаются путём удаления всех томов из них. Когда удаляется последний оставшийся том, массив останавливается, а метаданные удаляются с дисков:

[source, shell]
....
# graid delete raid/r0
....

[[geom-graid-unexpected]]
=== Удаление неожиданных массивов

Диски могут неожиданно содержать метаданные man:graid[8], оставшиеся от предыдущего использования или тестирования производителем. man:graid[8] обнаружит эти диски и создаст массив, что помешает доступу к отдельному диску. Для удаления нежелательных метаданных:

[.procedure]
. Загрузите систему. В меню загрузки выберите `2` для перехода в приглашение загрузчика. Введите:
+
[source, shell]
....
OK set kern.geom.raid.enable=0
OK boot
....
+
Система загрузится с отключенным man:graid[8].
. Создайте резервную копию всех данных на затронутом диске.
. В качестве обходного решения обнаружение массива man:graid[8] можно отключить, добавив
+
[.programlisting]
....
kern.geom.raid.enable=0
....
+
в файл [.filename]#/boot/loader.conf#.
+
Для постоянного удаления метаданных man:graid[8] с затронутого диска загрузитесь с установочного CD-ROM или USB-накопителя FreeBSD и выберите `Shell`. Используйте команду `status`, чтобы найти имя массива, обычно это `raid/r0`:
+
[source, shell]
....
# graid status
   Name   Status  Components
raid/r0  OPTIMAL  ada0 (ACTIVE (ACTIVE))
                  ada1 (ACTIVE (ACTIVE))
....
+
Удалите том по имени:
+
[source, shell]
....
# graid delete raid/r0
....
+
Если отображается более одного тома, повторите процесс для каждого тома. После удаления последнего массива том будет уничтожен.
+
Перезагрузите систему и проверьте данные, восстановив их из резервной копии при необходимости. После удаления метаданных запись `kern.geom.raid.enable=0` в файле [.filename]#/boot/loader.conf# также можно удалить.

[[geom-ggate]]
== Сетевые устройства GEOM Gate

GEOM предоставляет простой механизм для удаленного доступа к устройствам, таким как диски, компакт-диски и файловые системы, с использованием сетевого демона GEOM Gate - ggated. Система с устройством запускает серверный демон, который обрабатывает запросы от клиентов, использующих ggatec. Устройства не должны содержать конфиденциальных данных, так как соединение между клиентом и сервером не шифруется.

Аналогично NFS, который рассматривается в crossref:network-servers[network-nfs,"Network File System (NFS)"], ggated настраивается с использованием файла экспорта. Этот файл определяет, каким системам разрешён доступ к экспортируемым ресурсам и какой уровень доступа им предоставляется. Например, чтобы предоставить клиенту `192.168.1.5` права на чтение и запись для четвёртого раздела первого SCSI-диска, создайте файл [.filename]#/etc/gg.exports# со следующей строкой:

[.programlisting]
....
192.168.1.5 RW /dev/da0s4d
....

Перед экспортом устройства убедитесь, что оно не смонтировано. Затем запустите `ggated`:

[source, shell]
....
# ggated
....

Для указания альтернативного порта прослушивания или изменения расположения файла экспорта по умолчанию доступно несколько вариантов. Подробности см. в man:ggated[8].

Для доступа к экспортированному устройству на клиентской машине сначала используйте `ggatec`, указав IP-адрес сервера и имя экспортированного устройства. В случае успеха эта команда выведет имя устройства `ggate`, которое нужно смонтировать. Смонтируйте указанное имя устройства на свободную точку монтирования. В этом примере выполняется подключение к разделу [.filename]#/dev/da0s4d# на `192.168.1.1`, затем монтируется [.filename]#/dev/ggate0# в [.filename]#/mnt#:

[source, shell]
....
# ggatec create -o rw 192.168.1.1 /dev/da0s4d
ggate0
# mount /dev/ggate0 /mnt
....

Устройство на сервере теперь может быть доступно через [.filename]#/mnt# на клиенте. Для получения дополнительной информации о `ggatec` и нескольких примеров использования обратитесь к man:ggatec[8].

[NOTE]
====
Монтирование завершится ошибкой, если устройство в данный момент смонтировано на сервере или любом другом клиенте в сети. Если требуется одновременный доступ к сетевым ресурсам, используйте NFS.
====

Когда устройство больше не требуется, размонтируйте его с помощью `umount`, чтобы ресурс стал доступен другим клиентам.

[[geom-glabel]]
== Маркировка дисковых устройств

Во время инициализации системы ядро FreeBSD создает узлы устройств по мере их обнаружения. Такой метод поиска устройств вызывает некоторые проблемы. Например, что если новое дисковое устройство будет добавлено через USB? Вполне вероятно, что флеш-накопитель может получить имя устройства [.filename]#da0#, а исходное устройство [.filename]#da0# сместится на [.filename]#da1#. Это вызовет проблемы с монтированием файловых систем, если они указаны в [.filename]#/etc/fstab#, что также может помешать загрузке системы.

Одно из решений — последовательное подключение SCSI-устройств, чтобы новое устройство, добавленное к SCSI-карте, получало неиспользуемые номера. Но что делать с USB-устройствами, которые могут заменить основной SCSI-диск? Это происходит потому, что USB-устройства обычно определяются раньше SCSI-карты. Один из вариантов — подключать такие устройства только после загрузки системы. Другой способ — использовать только один ATA-диск и никогда не указывать SCSI-устройства в [.filename]#/etc/fstab#.

Лучшим решением будет использование `glabel` для маркировки дисковых устройств и применение меток в [.filename]#/etc/fstab#. Поскольку `glabel` сохраняет метку в последнем секторе заданного провайдера, метка останется неизменной после перезагрузки. Используя эту метку в качестве устройства, файловую систему можно всегда монтировать, независимо от того, через какой узел устройства к ней обращаются.

[NOTE]
====
`glabel` может создавать как временные, так и постоянные метки. Только постоянные метки сохраняются после перезагрузки. Дополнительную информацию о различиях между метками можно найти в man:glabel[8].
====

=== Типы меток и примеры

Постоянные метки могут быть общими или метками файловой системы. Постоянные метки файловой системы можно создать с помощью man:tunefs[8] или man:newfs[8]. Такие метки создаются в подкаталоге [.filename]#/dev# и именуются в соответствии с типом файловой системы. Например, метки файловой системы UFS2 будут созданы в [.filename]#/dev/ufs#. Общие постоянные метки можно создать с помощью `glabel label`. Они не привязаны к конкретной файловой системе и будут размещены в [.filename]#/dev/label#.

Временные метки удаляются при следующей перезагрузке. Эти метки создаются в [.filename]#/dev/label# и подходят для экспериментов. Временную метку можно создать с помощью `glabel create`.

Чтобы создать постоянную метку для файловой системы UFS2 без уничтожения данных, выполните следующую команду:

[source, shell]
....
# tunefs -L home /dev/da3
....

Теперь в [.filename]#/dev/ufs# должна существовать метка, которую можно добавить в [.filename]#/etc/fstab#:

[.programlisting]
....
/dev/ufs/home		/home            ufs     rw              2      2
....

[NOTE]
====
Файловая система не должна быть смонтирована при попытке запуска `tunefs`.
====

Теперь файловую систему можно смонтировать:

[source, shell]
....
# mount /home
....

Начиная с этого момента, при условии, что модуль ядра [.filename]#geom_label.ko# загружается при загрузке через [.filename]#/boot/loader.conf# или присутствует опция ядра `GEOM_LABEL`, изменение узла устройства не окажет негативного влияния на систему.

Файловые системы также могут быть созданы с меткой по умолчанию с использованием флага `-L` в `newfs`. Подробнее см. в man:newfs[8].

Следующая команда может быть использована для удаления метки:

[source, shell]
....
# glabel destroy home
....

Следующий пример показывает, как назначить метки разделам загрузочного диска.

.Разметка разделов на загрузочном диске
[example]
====
Постоянная маркировка разделов на загрузочном диске позволяет системе продолжать нормальную загрузку, даже если диск будет перемещен на другой контроллер или перенесен в другую систему. В этом примере предполагается, что используется один ATA-диск, который система в настоящее время распознает как [.filename]#ad0#. Также предполагается, что используется стандартная схема разделов FreeBSD с [.filename]#/#, [.filename]#/var#, [.filename]#/usr#, [.filename]#/tmp#, а также разделом подкачки.

Перезагрузите систему, и на запросе man:loader[8] нажмите kbd:[4], чтобы загрузиться в однопользовательском режиме. Затем введите следующие команды:

[source, shell]
....
# glabel label rootfs /dev/ad0s1a
GEOM_LABEL: Label for provider /dev/ad0s1a is label/rootfs
# glabel label var /dev/ad0s1d
GEOM_LABEL: Label for provider /dev/ad0s1d is label/var
# glabel label usr /dev/ad0s1f
GEOM_LABEL: Label for provider /dev/ad0s1f is label/usr
# glabel label tmp /dev/ad0s1e
GEOM_LABEL: Label for provider /dev/ad0s1e is label/tmp
# glabel label swap /dev/ad0s1b
GEOM_LABEL: Label for provider /dev/ad0s1b is label/swap
# exit
....

Система продолжит загрузку в многопользовательском режиме. После завершения загрузки отредактируйте файл [.filename]#/etc/fstab# и замените стандартные имена устройств на соответствующие метки. Итоговый файл [.filename]#/etc/fstab# будет выглядеть следующим образом:

[.programlisting]
....
# Device                Mountpoint      FStype  Options         Dump    Pass#
/dev/label/swap         none            swap    sw              0       0
/dev/label/rootfs       /               ufs     rw              1       1
/dev/label/tmp          /tmp            ufs     rw              2       2
/dev/label/usr          /usr            ufs     rw              2       2
/dev/label/var          /var            ufs     rw              2       2
....

Систему теперь можно перезагрузить. Если всё прошло успешно, она загрузится в обычном режиме, и команда `mount` покажет:

[source, shell]
....
# mount
/dev/label/rootfs on / (ufs, local)
devfs on /dev (devfs, local)
/dev/label/tmp on /tmp (ufs, local, soft-updates)
/dev/label/usr on /usr (ufs, local, soft-updates)
/dev/label/var on /var (ufs, local, soft-updates)
....

====

Класс man:glabel[8] поддерживает тип метки для файловых систем UFS, основанный на уникальном идентификаторе файловой системы, `ufsid`. Эти метки могут быть найдены в [.filename]#/dev/ufsid# и создаются автоматически во время запуска системы. Можно использовать метки `ufsid` для монтирования разделов с помощью [.filename]#/etc/fstab#. Для получения списка файловых систем и соответствующих им меток `ufsid` используйте команду `glabel status`:

[source, shell]
....
% glabel status
                  Name  Status  Components
ufsid/486b6fc38d330916     N/A  ad4s1d
ufsid/486b6fc16926168e     N/A  ad4s1f
....

В приведённом выше примере [.filename]#ad4s1d# соответствует [.filename]#/var#, а [.filename]#ad4s1f# — [.filename]#/usr#. Используя указанные значения `ufsid`, эти разделы можно смонтировать, добавив следующие строки в [.filename]#/etc/fstab#:

[.programlisting]
....
/dev/ufsid/486b6fc38d330916        /var        ufs        rw        2      2
/dev/ufsid/486b6fc16926168e        /usr        ufs        rw        2      2
....

Любые разделы с метками `ufsid` могут быть смонтированы таким образом, что устраняет необходимость вручную создавать постоянные метки, сохраняя при этом преимущества монтирования, независимого от имён устройств.

[[geom-gjournal]]
== Журналирование UFS через GEOM

Поддержка журналирования для файловых систем UFS доступна в FreeBSD. Реализация предоставляется через подсистему GEOM и настраивается с помощью `gjournal`. В отличие от других реализаций журналирования файловых систем, метод `gjournal` работает на уровне блоков и не является частью файловой системы. Это расширение GEOM.

В журналировании хранится журнал транзакций файловой системы, таких как изменения, составляющие полную операцию записи на диск, до того как метаданные и записи файлов будут записаны на диск. Этот журнал транзакций может быть позднее воспроизведен для повторного выполнения операций файловой системы, предотвращая её рассогласование.

Этот метод предоставляет ещё один механизм защиты от потери данных и нарушения целостности файловой системы. В отличие от мягких обновлений, которые отслеживают и контролируют обновления метаданных, и снимков, создающих образ файловой системы, журнал хранится в отдельном месте на диске, специально выделенном для этой задачи. Для повышения производительности журнал может храниться на другом диске. В такой конфигурации провайдер журнала или устройство хранения должны быть указаны после устройства, для которого включается ведение журнала.

Ядро [.filename]#GENERIC# предоставляет поддержку `gjournal`. Чтобы автоматически загружать модуль ядра [.filename]#geom_journal.ko# при загрузке, добавьте следующую строку в [.filename]#/boot/loader.conf#:

[.programlisting]
....
geom_journal_load="YES"
....

Если используется собственный вариант ядра, убедитесь, что следующая строка присутствует в конфигурационном файле ядра:

[.programlisting]
....
options	GEOM_JOURNAL
....

После загрузки модуля журнал можно создать на новой файловой системе, выполнив следующие шаги. В этом примере [.filename]#da4# — это новый SCSI-диск:

[source, shell]
....
# gjournal load
# gjournal label /dev/da4
....

Это загрузит модуль и создаст узел устройства [.filename]#/dev/da4.journal# на [.filename]#/dev/da4#.

Файловая система UFS теперь может быть создана на журналируемом устройстве, а затем смонтирована в существующей точке монтирования:

[source, shell]
....
# newfs -O 2 -J /dev/da4.journal
# mount /dev/da4.journal /mnt
....

[NOTE]
====
В случае нескольких разделов журнал будет создан для каждого отдельного раздела. Например, если [.filename]#ad4s1# и [.filename]#ad4s2# являются разделами, то `gjournal` создаст [.filename]#ad4s1.journal# и [.filename]#ad4s2.journal#.
====

Журналирование также можно включить на существующих файловых системах с помощью `tunefs`. Однако _всегда_ делайте резервную копию перед изменением существующей файловой системы. В большинстве случаев `gjournal` завершится с ошибкой, если не сможет создать журнал, но это не защищает от потери данных из-за неправильного использования `tunefs`. Дополнительную информацию об этих командах можно найти в man:gjournal[8] и man:tunefs[8].

Возможно журналирование загрузочного диска в системе FreeBSD. Подробные инструкции приведены в статье extref:{gjournal-desktop}[Реализация журналирования UFS на настольном ПК].
